{"cells":[{"cell_type":"markdown","id":"7176289c-b251-4037-972c-ab167c455a74","metadata":{"id":"7176289c-b251-4037-972c-ab167c455a74"},"source":["**Q1. What are the key tasks that machine learning entails? What does\n","data pre-processing imply?**\n","\n","**Following are the most common machine learning Key tasks**\n","\n","1.  **Data Gathering**: Any machine learning problem requires a lot of\n","    data for training/testing purposes. Identifying the right data\n","    sources and gathering data from these data sources is the key. Data\n","    could be found from databases, external agencies, the internet, etc.\n","\n","2.  **Data Pre-processing**: Before starting training the models, it is\n","    of utmost importance to prepare data appropriately.\n","\n","**As part of data pre-processing, some of the following is done:**\n","\n","1.  **Data cleaning**: Data cleaning requires one to identify attributes\n","    having not enough data or attributes which are not have variance.\n","    These data (rows and columns) need to be removed from the training\n","    data set.\n","\n","2.  **Missing data imputation**: Handling missing data using data\n","    imputation techniques such as replacing missing data with mean,\n","    median, or mode. Here is my post on this topic**: [<u>Replace\n","    missing values with mean, median or\n","    mode</u>](https://vitalflux.com/pandas-impute-missing-values-mean-median-mode/)**\n","\n","<!-- -->\n","\n","1.  **Exploratory Data Analysis (EDA)**: Once data is pre-processed, the\n","    next step is to perform exploratory data analysis to understand data\n","    distribution and relationships between/within the data.\n","\n","**Some of the followings are performed as part of EDA:**\n","\n","1.  Correlation analysis\n","\n","2.  Multicollinearity analysis\n","\n","3.  Data distribution analysis\n","\n","<!-- -->\n","\n","1.  **Feature Engineering**: Feature engineering is one of the critical\n","    tasks which would be used when building machine learning models.\n","    Feature engineering is important because selecting the right\n","    features would not only help build models of higher accuracy but\n","    also help achieve objectives related to building simpler models,\n","    reducing overfitting, etc. Feature engineering includes tasks such\n","    as deriving features from raw features, identifying important\n","    features, feature extraction, and feature selection.\n","\n","**The following are some of the techniques which could be used for\n","feature selection:**\n","\n","1.  Filter methods help in selecting features based on the outcomes of\n","    statistical tests. The following are some of the statistical tests\n","    which are used:\n","\n","    1.  Pearson’s correlation\n","\n","    2.  Linear discriminant analysis (LDA)\n","\n","    3.  Analysis of Variance (ANOVA)\n","\n","    4.  Chi-square tests\n","\n","2.  Wrapper methods help in feature selection by using a subset of\n","    features and determining the model accuracy. The following are some\n","    of the algorithms used:\n","\n","    1.  Forward selection\n","\n","    2.  Backward elimination\n","\n","    3.  Recursive feature elimination\n","\n","3.  Regularization techniques penalize one or more features\n","    appropriately to come up with most important features. The following\n","    are some of the algorithms used:\n","\n","    1.  LASSO (L1) regularization\n","\n","    2.  Ridge (L2) regularization\n","\n","    3.  Elastic net regularization\n","\n","    4.  Regularization with classification algorithms such as Logistic\n","        regression, SVM, etc.\n","\n","    5.  \n","\n","<!-- -->\n","\n","1.  **Training Models: **Once some of the features are determined, then\n","    comes training models with data related to those features. One\n","    popular algorithm used for while training regression models\n","    is **Gradient Descent** which helps us to find optimal parameter\n","    values in order to minimize our cost function (also known as error\n","    rate). In this method, we start with random initial parameter values\n","    and then gradually update them by taking small steps until we reach\n","    an optimal solution. This iterative process helps us reduce error\n","    rates over time and ultimately provide better predictions for our\n","    target variable.\n","\n","2.  **Model selection / Algorithm selection**: Many times, there are\n","    multiple models which are trained using different algorithms. One of\n","    the important tasks is to select the most optimal models for\n","    deploying them in production. **Hyperparameter tuning** is the most\n","    common task performed as part of model selection. Also, if there are\n","    two models trained using different algorithms which have similar\n","    performance, then one also needs to perform algorithm selection.\n","\n","3.  **Testing and matching**: Testing and matching tasks relate to\n","    comparing data sets. Following are some of the methods that could be\n","    used for such kinds of problems:\n","\n","    1.  Minimum spanning tree\n","\n","    2.  Bipartite cross-matching\n","\n","    3.  N-point correlation\n","\n","4.  **Model monitoring**: Once the models are trained and deployed, they\n","    require to be monitored at regular intervals. Monitoring models\n","    require the processing actual values and predicted values and\n","    measuring the model performance based on appropriate metrics.\n","\n","5.  **Model retraining**: In case, the model performance degrades, the\n","    models are required to be retrained.  The following gets done as\n","    part of model retraining:\n","\n","    1.  New features get determined\n","\n","    2.  New algorithms can be used\n","\n","    3.  Hyperparameters can get tuned\n","\n","    4.  Model ensembles may get deployed\n","\n","**What does data pre-processing imply?**\n","\n","Data pre-processing is a step in the data mining and data analysis\n","process that takes raw data and transforms it into a format that can be\n","understood and analysed by computers and machine learning.\n","\n","-   **It improves accuracy and reliability.** Pre-processing data\n","    removes missing or inconsistent data values resulting from human or\n","    computer error, which can improve the accuracy and quality of a\n","    dataset, making it more reliable.\n","\n","-   **It makes data consistent.** When collecting data, it's possible to\n","    have data duplicates, and discarding them during pre-processing can\n","    ensure the data values for analysis are consistent, which helps\n","    produce accurate results.\n","\n","-   **It increases the data's algorithm readability.** Pre-processing\n","    enhances the data's quality and makes it easier for machine learning\n","    algorithms to read, use, and interpret it.\n","\n","**Q2. Describe quantitative and qualitative data in depth. Make a\n","distinction between the two.**\n","\n","When it comes to conducting data research, you’ll need different\n","collection, hypotheses and analysis methods, so it’s important to\n","understand the key **differences between quantitative and qualitative\n","data:**\n","\n","-   **Quantitative data** is numbers-based, countable, or\n","    measurable. **Qualitative data** is interpretation-based,\n","    descriptive, and relating to language.\n","\n","-   **Quantitative data** tells us how many, how much, or how often in\n","    calculations. **Qualitative data** can help us to understand why,\n","    how, or what happened behind certain behaviours.\n","\n","-   **Quantitative data** is fixed and universal. **Qualitative\n","    data** is subjective and unique.\n","\n","-   **Quantitative research** methods are measuring and\n","    counting. **Qualitative research** methods are interviewing and\n","    observing.\n","\n","-   **Quantitative data** is analysed using statistical\n","    analysis. **Qualitative data** is analysed by grouping the data into\n","    categories and themes.\n","\n","> <img\n","> src=\"attachment:vertopal_f9e24c8111ac4f8bb0a33dbab1109790/media/image1.png\"\n","> style=\"width:6.97128in;height:3.39563in\"\n","> alt=\"Qualtitative vs quantitative examples\" />\n","\n","**Q3. Create a basic data collection that includes some sample records.\n","Have at least one attribute from each of the machine learning data\n","types.**\n","\n","**Customer Information**\n","\n","<img\n","src=\"attachment:vertopal_f9e24c8111ac4f8bb0a33dbab1109790/media/image2.png\"\n","style=\"width:7.16667in;height:2.23604in\" />\n","\n","**Explanation of attributes:**\n","\n","**1.** **Customer ID:** An identifier for each customer in the data\n","collection.\n","\n","**2.** **Age:** The age of the customer in years. It represents a\n","numerical data type.\n","\n","**3. Gender:** The gender of the customer. It represents a categorical\n","data type.\n","\n","**4. Income (USD):** The annual income of the customer in US dollars. It\n","represents a numerical data type.\n","\n","**5. Purchase Category:** The category of the customer's recent\n","purchase. It represents a categorical data type.\n","\n","**6. Purchase Amount (USD):** The amount spent by the customer in the\n","recent purchase in US dollars. It represents a numerical data type.\n","\n","**Q4. What are the various causes of machine learning data issues? What\n","are the ramifications?**\n","\n","Machine learning data can suffer from various issues that can impact the\n","performance and reliability of the models. **Here are some common causes\n","of machine learning data issues and their potential ramifications:**\n","\n","1.  **Insufficient Data Quantity:** Having an inadequate amount of data\n","    can lead to poor model performance. Insufficient data may result in\n","    models that are underfitting, lacking generalization, or prone to\n","    overfitting small patterns in the data.\n","\n","**Ramifications:** Models may be inaccurate, unstable, or unreliable,\n","leading to poor predictions or incorrect decisions.\n","\n","1.  **Imbalanced Data:** Imbalanced data occurs when the distribution of\n","    classes or target variables is uneven, with one class significantly\n","    outnumbering the others. This issue is common in binary\n","    classification problems where one class is rare.\n","\n","**Ramifications:** Models may become biased toward the majority class,\n","leading to poor performance on the minority class. They may struggle to\n","generalize well for the underrepresented class, resulting in false\n","negatives or positives.\n","\n","1.  **Missing Data:** Missing data refers to the absence of certain\n","    attribute values in the dataset. Missing data can occur due to\n","    various reasons, such as data collection errors or incomplete\n","    records.\n","\n","> **Ramifications:** Missing data can lead to biased or incomplete\n","> models. The absence of important information may hinder the model's\n","> ability to make accurate predictions or decisions.\n","\n","1.  **Noisy Data:** Noisy data contains errors, inconsistencies, or\n","    outliers that do not reflect the underlying patterns. It can arise\n","    due to measurement errors, data corruption, or human error during\n","    data collection.\n","\n","**Ramifications:** Noisy data can distort the model's learning process,\n","leading to inaccurate predictions or unstable performance. Models may\n","capture noise as signal, reducing their generalization capabilities.\n","\n","1.  **Biased Data:** Biased data reflects a skewed representation of the\n","    population being modelled. Bias can be introduced during data\n","    collection or sampling, leading to biased model outputs.\n","\n","> **Ramifications:** Biased data can perpetuate and amplify existing\n","> biases, leading to unfair or discriminatory outcomes. Models may\n","> reinforce societal prejudices or discrimination present in the\n","> training data.\n","\n","1.  **Data Drift:** Data drift occurs when the statistical properties of\n","    the input data change over time. This change can be due to evolving\n","    user behaviour, shifts in data sources, or changes in the underlying\n","    distribution.\n","\n","> **Ramifications:** Data drift can deteriorate the model's performance\n","> over time. Models trained on outdated data may fail to adapt to the\n","> new patterns or trends in the data, leading to degraded accuracy or\n","> reliability.\n","\n","**Q5. Demonstrate various approaches to categorical data exploration\n","with appropriate examples.**\n","\n","Exploring categorical data is an important step in understanding the\n","distribution, relationships, and patterns within a dataset. Here are\n","some common approaches to categorical data exploration, **along with\n","appropriate examples:**\n","\n","1.  **Frequency Distribution:** Analysing the frequency distribution of\n","    > categorical variables helps understand the distribution of\n","    > different categories and their relative proportions.\n","\n","> **Example:** Consider a dataset of customer reviews for a product. You\n","> can create a frequency distribution table to show the count or\n","> percentage of reviews in each sentiment category, such as positive,\n","> neutral, and negative.\n","\n","<img\n","src=\"attachment:vertopal_f9e24c8111ac4f8bb0a33dbab1109790/media/image3.png\"\n","style=\"width:6.92708in;height:1.59375in\" />\n","\n","1.  **Bar Plot:** Bar plots are effective visualizations to display the\n","    > frequencies or proportions of different categories in a\n","    > categorical variable.\n","\n","> **Example:** Using the above sentiment categories, you can create a\n","> bar plot to visualize the distribution of sentiment in the customer\n","> reviews, with the x-axis representing sentiment categories and the\n","> y-axis representing the count or percentage.\n","\n","1.  **Cross-tabulation (Contingency Table):** Cross-tabulation allows\n","    > you to explore the relationship between two categorical variables\n","    > by tabulating the frequencies or proportions in a table.\n","\n","> **Example:** Suppose you have a dataset of students and their\n","> performance levels (high, medium, low) in two subjects (math and\n","> science). You can create a cross-tabulation table to analyse the\n","> distribution of performance levels across subjects.\n","\n","<img\n","src=\"attachment:vertopal_f9e24c8111ac4f8bb0a33dbab1109790/media/image4.png\"\n","style=\"width:6.78125in;height:1.13542in\" />\n","\n","1.  **Stacked Bar Plot:** Stacked bar plots visualize the distribution\n","    of a categorical variable while comparing sub-categories or groups\n","    within it.\n","\n","**Example:** Using the previous example of student performance, you can\n","create a stacked bar plot to show the distribution of performance levels\n","(high, medium, low) in math and science subjects.3\n","\n","1.  **Heatmap:** Heatmaps are useful for visualizing the relationships\n","    between multiple categorical variables, displaying the frequencies\n","    or proportions in a color-coded matrix.\n","\n","**Example:** Let's consider a dataset of online retail transactions. You\n","can create a heatmap to explore the relationship between product\n","categories and customer segments, where each cell represents the count\n","or proportion of transactions.\n","\n","**Q6. How would the learning activity be affected if certain variables\n","have missing values? Having said that, what can be done about it?**\n","\n","The presence of missing values in variables can have a significant\n","impact on the learning activity and the performance of machine learning\n","models. Here are some ways in which missing values can affect the\n","learning process:\n","\n","1.  **Biased Analysis:** If missing values are not handled\n","    appropriately, it can introduce bias in the analysis. This bias can\n","    impact the validity of conclusions drawn from the data and lead to\n","    incorrect predictions or decisions.\n","\n","2.  **Incomplete Data:** Missing values can result in incomplete data,\n","    which can hinder the learning process. Models may not have enough\n","    information to learn patterns and make accurate predictions, leading\n","    to reduced performance.\n","\n","3.  **Data Skewness:** The distribution of missing values itself can\n","    introduce skewness in the data. Variables with a high percentage of\n","    missing values may not accurately represent the overall population,\n","    leading to skewed results and biased models.\n","\n","**To address missing values, several techniques can be employed:**\n","\n","1.  **Deletion:** If the missing values are minimal and do not\n","    significantly impact the dataset, you can consider deleting the rows\n","    or columns with missing values. However, this approach can lead to\n","    loss of information and reduced sample size.\n","\n","2.  **Imputation:** Imputation involves filling in the missing values\n","    with estimated or imputed values. Common imputation methods include\n","    mean, median, mode imputation for numerical variables, and using the\n","    most frequent category for categorical variables. More advanced\n","    techniques like regression imputation or multiple imputation can\n","    also be used.\n","\n","3.  **Indicator Variable:** Another approach is to create an indicator\n","    variable that flags whether a value is missing or not. This approach\n","    preserves the information about missingness and allows the model to\n","    learn patterns associated with missing values.\n","\n","4.  **Advanced Imputation Techniques:** Advanced imputation methods,\n","    such as k-nearest neighbors (KNN) imputation,\n","    expectation-maximization (EM) algorithm, or predictive\n","    modeling-based imputation, can be employed to estimate missing\n","    values based on the relationships with other variables.\n","\n","5.  **Domain Knowledge:** Leveraging domain knowledge and expert\n","    insights can help make informed decisions on how to handle missing\n","    values. Domain experts may have insights into the reasons behind\n","    missing values and can provide guidance on appropriate imputation\n","    methods.\n","\n","**Q7. Describe the various methods for dealing with missing data values\n","in depth.**\n","\n","Dealing with missing data values is a crucial step in data\n","pre-processing to ensure the reliability and accuracy of analyses and\n","machine learning models. **Here are several methods for handling missing\n","data values in depth:**\n","\n","**1. Deletion Techniques:**\n","\n","-   Listwise Deletion (Complete Case Analysis): In this approach, any\n","    record with a missing value in any variable is entirely removed from\n","    the dataset. It is a simple and straightforward method but can lead\n","    to a loss of valuable data.\n","\n","-   Pairwise Deletion: In this approach, missing values are ignored only\n","    for specific analyses or calculations involving variables with\n","    missing values. This method allows for maximum utilization of\n","    available data but can introduce biases in certain analyses.\n","\n","**2. Mean/Mode/Median Imputation:**\n","\n","-   Mean Imputation: Missing numerical values are replaced with the mean\n","    value of the non-missing values for that variable.\n","\n","-   Mode Imputation: Missing categorical values are replaced with the\n","    mode (most frequent category) of the non-missing values for that\n","    variable.\n","\n","-   Median Imputation: Missing numerical values are replaced with the\n","    median value of the non-missing values for that variable.\n","\n","**3. Hot Deck Imputation:**\n","\n","-   Hot Deck Imputation involves replacing missing values with similar\n","    values from the same dataset. The similar values can be selected\n","    based on various criteria like nearest neighbour matching or random\n","    selection from similar records.\n","\n","-   In the simplest form, hot deck imputation replaces a missing value\n","    with the observed value from a randomly selected record that has\n","    similar characteristics (e.g., age, gender, etc.) as the record with\n","    the missing value.\n","\n","**4. Regression Imputation:**\n","\n","-   Regression Imputation involves using regression models to predict\n","    missing values based on the relationships with other variables. A\n","    regression model is trained using the non-missing values, and the\n","    predicted values are used to impute the missing values.\n","\n","-   Simple linear regression or multiple regression can be used\n","    depending on the nature of the variables involved.\n","\n","**5. Multiple Imputation:**\n","\n","-   Multiple Imputation generates multiple plausible values for each\n","    missing value, creating several complete datasets. The missing\n","    values are imputed using statistical models, taking into account the\n","    uncertainty associated with the imputations.\n","\n","-   Each complete dataset is analysed separately, and the results are\n","    combined using specific rules for pooling the estimates and\n","    accounting for the variability introduced by the imputation process.\n","\n","**6. Machine Learning Imputation:**\n","\n","-   Machine learning algorithms, such as k-nearest neighbours (KNN),\n","    decision trees, or random forests, can be used to predict missing\n","    values based on the relationship with other variables.\n","\n","-   These algorithms learn patterns from the available data and use them\n","    to impute the missing values.\n","\n","**7. Domain-Specific Imputation:**\n","\n","-   In certain cases, domain knowledge or expert insights can guide the\n","    imputation process. For example, if missing values are related to a\n","    specific condition or event, an expert may suggest a specific\n","    imputation strategy based on the context.\n","\n","**Q8. What are the various data pre-processing techniques? Explain\n","dimensionality reduction and function selection in a few words.**\n","\n","Data pre-processing techniques are applied to transform raw data into a\n","suitable format for analysis and modelling. **Some common data\n","pre-processing techniques include:**\n","\n","**1. Data Cleaning:** Removing or correcting errors, inconsistencies,\n","and outliers in the data to ensure data quality and reliability.\n","\n","**2. Data Integration:** Combining data from multiple sources into a\n","unified dataset to provide a comprehensive view for analysis.\n","\n","**3. Data Transformation:** Converting data into a standardized format\n","or scale to facilitate analysis and modelling**. Examples** include\n","normalization, logarithmic transformation, and categorical variable\n","encoding.\n","\n","**4. Data Discretization:** Grouping continuous or interval data into\n","discrete intervals or bins to simplify analysis and reduce the impact of\n","minor fluctuations.\n","\n","**5. Feature Selection:** Selecting a subset of relevant features from\n","the original set of variables to improve model performance, reduce\n","overfitting, and enhance interpretability.\n","\n","**6. Dimensionality Reduction:** Reducing the number of variables\n","(dimensions) in a dataset while preserving its essential information.\n","This helps to address the curse of dimensionality and improve model\n","efficiency and interpretability.\n","\n","**Now, let's delve into dimensionality reduction and feature selection\n","in a few words:**\n","\n","**1. Dimensionality Reduction:** Dimensionality reduction refers to the\n","process of reducing the number of variables (dimensions) in a dataset\n","while retaining important information. It aims to simplify complex\n","datasets, remove redundant or irrelevant features, and improve\n","computational efficiency.\n","\n","-   Techniques such as Principal Component Analysis (PCA) and Linear\n","    Discriminant Analysis (LDA) are commonly used for dimensionality\n","    reduction. These methods transform the original variables into a\n","    lower-dimensional space, capturing the maximum amount of variation\n","    or discriminative information in the data.\n","\n","-   By reducing dimensionality, models can be trained more efficiently,\n","    as the number of variables to consider decreases. Dimensionality\n","    reduction also helps mitigate issues like overfitting, improves\n","    visualization capabilities, and enhances interpretability by\n","    focusing on the most informative aspects of the data.\n","\n","**2. Feature Selection:** Feature selection involves choosing a subset\n","of relevant features from the original set of variables to improve model\n","performance and reduce complexity. The goal is to identify the most\n","informative features that contribute the most to the target variable\n","while discarding redundant or irrelevant features.\n","\n","-   Feature selection methods can be categorized into three types:\n","    filter methods, wrapper methods, and embedded methods. Filter\n","    methods evaluate the features based on their statistical properties,\n","    such as correlation or information gain. Wrapper methods use a\n","    specific machine learning algorithm to assess the subsets of\n","    features based on their impact on model performance. Embedded\n","    methods incorporate feature selection as an integral part of the\n","    learning algorithm itself.\n","\n","-   Feature selection helps in reducing overfitting, improving model\n","    interpretability, reducing training time, and addressing the curse\n","    of dimensionality. It simplifies the model by focusing on the most\n","    relevant features, leading to better generalization and predictive\n","    accuracy.\n","\n","**Q9. (i) What is the IQR? What criteria are used to assess it?**\n","\n","The interquartile range defines the difference between the third and the\n","first quartile. Quartiles are the partitioned values that divide the\n","whole series into 4 equal parts. So, there are 3 quartiles. First\n","Quartile is denoted by Q<sub>1 </sub>known as the lower quartile, the\n","second Quartile is denoted by Q<sub>2</sub> and the third Quartile is\n","denoted by Q<sub>3</sub> known as the upper quartile. Therefore, the\n","interquartile range is equal to the upper quartile minus lower quartile.\n","\n","## Interquartile Range Formula\n","\n","The difference between the upper and lower quartile is known as the\n","interquartile range. The formula for the interquartile range is given\n","below\n","\n","**Interquartile range = Upper Quartile – Lower Quartile =\n","Q<sub>­3</sub> – Q<sub>­1</sub>**\n","\n","where Q<sub>1</sub> is the first quartile and Q<sub>3</sub> is the third\n","quartile of the series.\n","\n","The below figure shows the occurrence of median and interquartile range\n","for the data set.\n","\n","<img\n","src=\"attachment:vertopal_f9e24c8111ac4f8bb0a33dbab1109790/media/image5.png\"\n","style=\"width:5.21875in;height:3.09375in\" alt=\"Interquartile range\" />\n","\n","## Semi Interquartile Range\n","\n","The semi-interquartile range is defined as the [measures of\n","dispersion](https://byjus.com/maths/dispersion/). Semi interquartile\n","range also is defined as half of the interquartile range. It is computed\n","as one half the difference between the 75th percentile (Q<sub>3</sub>)\n","and the 25th percentile (Q<sub>1</sub>). The semi-interquartile range is\n","one-half of the difference between the first and third quartiles. The\n","Formula for Semi Interquartile Range is\n","\n","**Semi Interquartile Range = (Q<sub>3</sub>– Q<sub>1</sub>) / 2**\n","\n","### Median and Interquartile Range\n","\n","The median is the middle value of the distribution of the given data.\n","The interquartile range (IQR) is the range of values that resides in the\n","middle of the scores. When a distribution is skewed, and the median is\n","used instead of the mean to show a [central\n","tendency](https://byjus.com/maths/central-tendency/), the appropriate\n","measure of variability is the Interquartile range.\n","\n","Q<sub>1 </sub>– Lower Quartile Part\n","\n","Q<sub>2</sub> – Median\n","\n","Q<sub>3</sub> – Upper Quartile Part\n","\n","It is a measure of dispersion based on the lower and upper quartile.\n","Quartile deviation is obtained from interquartile range on dividing by\n","2, hence also known as semi-interquartile range.\n","\n","**Q9. (ii) Describe the various components of a box plot in detail? When\n","will the lower whisker surpass the upper whisker in length? How can box\n","plots be used to identify outliers?**\n","\n","A box plot, also known as a box-and-whisker plot, is a graphical\n","representation of a dataset that provides a summary of its distribution,\n","including measures of central tendency and dispersion. It consists of\n","several components, each conveying different information about the\n","dataset. Let's discuss each component in detail:\n","\n","1.  **Median (Q2):** The median represents the middle value of the\n","    dataset when it is sorted in ascending order. It divides the data\n","    into two halves, with 50% of the values falling below it and 50%\n","    above it. In a box plot, the median is depicted as a horizontal line\n","    inside the box.\n","\n","2.  **Interquartile Range (IQR):** The IQR measures the spread of the\n","    central 50% of the data. It is calculated as the difference between\n","    the third quartile (Q3) and the first quartile (Q1). The quartiles\n","    divide the dataset into four equal parts, with Q1 representing the\n","    25th percentile and Q3 representing the 75th percentile. The IQR is\n","    depicted as the height of the box in the box plot.\n","\n","3.  **Lower Whisker:** The lower whisker extends from the bottom of the\n","    box to the smallest data point that is still within the \"whisker\n","    length\" of the lower fence. The lower fence is typically calculated\n","    as Q1 - 1.5 \\* IQR or as the minimum value, whichever is smaller.\n","    Any data points below the lower fence are considered outliers and\n","    are plotted individually as points or asterisks.\n","\n","4.  **Upper Whisker:** The upper whisker extends from the top of the box\n","    to the largest data point that is still within the \"whisker length\"\n","    of the upper fence. The upper fence is typically calculated as Q3 +\n","    1.5 \\* IQR or as the maximum value, whichever is larger. Any data\n","    points above the upper fence are considered outliers and are plotted\n","    individually as points or asterisks.\n","\n","5.  **Outliers:** Outliers are data points that fall outside the\n","    whiskers. They are plotted individually in a box plot and can be\n","    identified as values that are located significantly away from the\n","    box and whiskers. Outliers can provide valuable insights into\n","    unusual or unexpected observations in the dataset.\n","\n","The length of the lower whisker will surpass the upper whisker when the\n","minimum value of the dataset lies below the lower fence and is\n","considered an outlier. In this case, the lower whisker will extend up to\n","the minimum value, while the upper whisker remains within the upper\n","fence and is limited to the maximum value or the data point closest to\n","it within the whisker length.\n","\n","Box plots can be useful for identifying outliers in a dataset. Any data\n","points that fall outside the whiskers are considered potential outliers.\n","By visually inspecting the box plot, you can quickly identify\n","observations that deviate significantly from the central tendency and\n","distribution of the rest of the data. Outliers might indicate\n","measurement errors, data entry mistakes, or genuine extreme values that\n","warrant further investigation or consideration in the analysis.\n","\n","**Q10. Make brief notes on any two of the following:**\n","\n","**1. Data collected at regular intervals**\n","\n","When data is collected at regular intervals, it is often referred to as\n","time series data or regularly spaced data. Here are some key points to\n","keep in mind about this type of data:\n","\n","1**. Definition:** Data collected at regular intervals refers to\n","observations or measurements that are recorded at fixed time intervals.\n","These intervals can be equally spaced, such as hourly, daily, weekly, or\n","monthly, or they can have other predetermined intervals based on the\n","nature of the data.\n","\n","2\\. **Time axis:** Time is typically plotted on the x-axis when\n","visualizing the data. The time intervals are represented along the axis,\n","allowing for the examination of patterns and trends over time.\n","\n","3\\. **Temporal relationships:** Regularly collected data provides\n","insights into how variables change over time. It allows for the\n","exploration of temporal relationships, including seasonality, trends,\n","cycles, and other patterns that may emerge.\n","\n","4\\. **Seasonality:** Seasonality refers to patterns that repeat at\n","regular intervals within a year, such as sales increasing during holiday\n","seasons or temperature fluctuations across the four seasons. Regularly\n","collected data can reveal seasonality effects, helping to understand and\n","forecast future trends.\n","\n","5\\. **Trend analysis:** By examining data collected at regular\n","intervals, it becomes possible to identify long-term trends or changes\n","over time. Trend analysis helps to understand whether a variable is\n","increasing, decreasing, or staying relatively stable over an extended\n","period.\n","\n","6\\. **Smoothing and aggregation:** Sometimes, it may be necessary to\n","smooth or aggregate the data to remove noise or reduce the granularity\n","of the observations. This can be done by averaging values over larger\n","time intervals or applying statistical techniques to reduce random\n","fluctuations.\n","\n","7\\. **Forecasting:** Regularly collected data is often used for\n","forecasting future values. Various time series forecasting methods, such\n","as ARIMA, exponential smoothing, or machine learning algorithms, can be\n","applied to predict future values based on historical patterns.\n","\n","8\\. **Analysis techniques:** Time series analysis techniques, including\n","autocorrelation, spectral analysis, and decomposition, can be employed\n","to uncover hidden patterns, periodicities, and relationships within the\n","data.\n","\n","**2. The gap between the quartiles**\n","\n","The gap between the quartiles, also known as the interquartile range\n","(IQR), is a measure of the spread or dispersion of the middle 50% of the\n","data. Here are some brief notes about the gap between the quartiles:\n","\n","**1. Definition:** The interquartile range (IQR) is calculated as the\n","difference between the third quartile (Q3) and the first quartile (Q1).\n","It represents the range of values that contain the middle 50% of the\n","dataset.\n","\n","**2. Measure of dispersion:** The IQR provides a measure of how spread\n","out the values are around the median. It is less influenced by extreme\n","values or outliers compared to the range or standard deviation.\n","\n","**3. Robustness:** The IQR is a robust statistic, meaning it is less\n","affected by extreme values. It focuses on the middle portion of the data\n","and is not influenced by outliers that may exist in the dataset.\n","\n","**4. Interpretation:** A larger IQR indicates a greater spread or\n","variability in the dataset, suggesting that the data points are more\n","widely dispersed. Conversely, a smaller IQR suggests a narrower range of\n","values and less variability within the dataset.\n","\n","**5. Outliers:** The IQR is commonly used to identify outliers. Data\n","points that fall below Q1 - 1.5 \\* IQR or above Q3 + 1.5 \\* IQR are\n","considered potential outliers. These values are plotted individually in\n","a box plot, outside the whiskers.\n","\n","**6. Comparisons:** The IQR allows for comparisons between different\n","datasets. A larger IQR in one dataset compared to another indicates a\n","greater spread or dispersion of values in the former.\n","\n","**7. Box plot representation:** In a box plot, the IQR is visually\n","represented as the height of the box. The box contains the middle 50% of\n","the data, with the median marked as a horizontal line within the box.\n","\n","**Q11. Make a comparison between:**\n","\n","**1. Data with nominal and ordinal values**\n","\n","**The main differences between Nominal Data and Ordinal Data are:**\n","\n","-   While Nominal Data is classified without any intrinsic ordering or\n","    > rank, Ordinal Data has some predetermined or natural order.\n","\n","-   Nominal data is qualitative or categorical data, while Ordinal data\n","    > is considered “in-between” qualitative and quantitative data.\n","\n","-   Nominal data do not provide any quantitative value, and you cannot\n","    > perform numeric operations with them or compare them with one\n","    > another. However, Ordinal data provide sequence, and it is\n","    > possible to assign numbers to the data. No numeric operations can\n","    > be performed. But ordinal data makes it possible to compare one\n","    > item with another in terms of ranking.\n","\n","-   Example of Nominal Data – Eye colour, Gender; Example of Ordinal\n","    > data – Customer Feedback, Economic Status\n","\n","<img\n","src=\"attachment:vertopal_f9e24c8111ac4f8bb0a33dbab1109790/media/image6.jpeg\"\n","style=\"width:3.20833in;height:1.53125in\" />\n","\n","When comparing data with nominal and ordinal values, there are\n","significant differences in their characteristics and the level of\n","information they provide. Here's a comparison between these two types of\n","data:\n","\n","1.  **Definition:**\n","\n","-   Nominal data: Nominal data represents categories or labels without\n","    any inherent order or numerical value. It classifies data into\n","    distinct groups or classes.\n","\n","-   Ordinal data: Ordinal data also represents categories or labels, but\n","    these categories have a natural order or ranking associated with\n","    them.\n","\n","1.  **Level of measurement:**\n","\n","-   Nominal data: Nominal data is at the lowest level of measurement. It\n","    only allows for categorization and identification of different\n","    groups. It lacks the ability to quantify or measure the degree of\n","    difference between categories.\n","\n","-   Ordinal data: Ordinal data is at a higher level of measurement\n","    compared to nominal data. It not only categorizes but also\n","    introduces an inherent order or ranking among the categories. The\n","    order represents a relative ranking or preference but does not\n","    specify the exact differences or distances between categories.\n","\n","1.  **Data analysis:**\n","\n","-   Nominal data: Nominal data is often analysed using frequency counts\n","    and percentages. It is suitable for calculating mode, conducting\n","    chi-square tests, or examining association between categories.\n","\n","-   Ordinal data: Ordinal data analysis includes methods such as rank\n","    correlation, cumulative frequency, or ordinal regression. These\n","    techniques account for the inherent order of the categories and can\n","    provide insights into the direction and strength of relationships.\n","\n","1.  **Information conveyed:**\n","\n","-   Nominal data: Nominal data primarily provides information about the\n","    frequencies and proportions of different categories. It answers\n","    questions of \"how many\" or \"what proportion\" belong to each\n","    category.\n","\n","-   Ordinal data: In addition to the information about frequencies and\n","    proportions, ordinal data also captures the order or ranking of\n","    categories. It provides insights into relative preferences or\n","    rankings but doesn't quantify the exact differences between the\n","    categories.\n","\n","1.  **Mathematical operations:**\n","\n","-   Nominal data: Nominal data does not support mathematical operations\n","    like addition, subtraction, or multiplication since the categories\n","    lack numerical values.\n","\n","-   Ordinal data: Ordinal data generally doesn't support mathematical\n","    operations either, as the differences between categories are not\n","    precisely defined or quantified.\n","\n","**2. Histogram and box plot**\n","\n","**Histogram and box plot** are both commonly used graphical\n","representations in statistics to summarize and visualize data. While\n","they serve similar purposes, there are distinct differences between the\n","two. Let's explore them:\n","\n","**Histogram:** A histogram is a graphical display that represents the\n","distribution of numerical data. It consists of a series of bars, where\n","each bar represents a specific range or bin of values. The horizontal\n","axis represents the range of values, divided into bins, and the vertical\n","axis represents the frequency or count of observations falling within\n","each bin. The height of each bar corresponds to the frequency or\n","relative frequency of observations in that bin.\n","\n","Histograms are useful for understanding the shape, central tendency, and\n","spread of a dataset. They allow you to visualize the frequency\n","distribution and identify patterns such as skewness, multimodality, or\n","outliers. Histograms provide a continuous representation of data, as the\n","bars are adjacent and have no gaps between them.\n","\n","**Box Plot:** A box plot, also known as a box-and-whisker plot, is a\n","graphical summary of a dataset's distribution. It presents the\n","five-number summary: minimum, first quartile (25th percentile), median\n","(50th percentile), third quartile (75th percentile), and maximum. The\n","plot consists of a box, which spans the interquartile range (IQR) and\n","contains the median. Two lines, called whiskers, extend from the box to\n","represent the minimum and maximum values within a certain range.\n","Additionally, box plots may include outliers plotted as individual\n","points.\n","\n","Box plots are particularly useful for comparing multiple datasets or\n","visualizing the spread and symmetry of a single dataset. They provide a\n","concise summary of the data's central tendency, dispersion, skewness,\n","and presence of outliers. Box plots are especially valuable when dealing\n","with large datasets or when comparing distributions across different\n","groups.\n","\n","**Differences:**\n","\n","1.  **Representation:** Histograms display the frequency distribution of\n","    values using adjacent bars, while box plots summarize the data's\n","    distribution through the five-number summary and outliers.\n","\n","2.  **Continuous vs. summary:** Histograms provide a detailed\n","    representation of the entire dataset, whereas box plots provide a\n","    condensed summary of the dataset's key statistics.\n","\n","3.  **Skewness:** Histograms can show the shape of the distribution,\n","    including skewness (asymmetric data), while box plots do not\n","    explicitly display the distribution's shape.\n","\n","4.  **Outliers:** While histograms can reveal outliers through unusual\n","    bars, box plots explicitly identify and display outliers as\n","    individual points.\n","\n","5.  **Comparison:** Box plots are often used to compare multiple\n","    datasets side by side, making it easier to understand the\n","    differences in their distributions. Histograms are typically used to\n","    analyse a single dataset's distribution.\n","\n","**3. The average and median**\n","\n","The average and median are both measures of central tendency in a\n","dataset, but they are calculated differently and provide different\n","insights into the data.\n","\n","**Average (Mean):** The average, also known as the mean, is calculated\n","by summing up all the values in a dataset and dividing the sum by the\n","total number of observations. It is influenced by every value in the\n","dataset, making it sensitive to extreme values or outliers. The formula\n","for calculating the average is:\n","\n","Average = (Sum of all values) / (Number of observations)\n","\n","The average provides a measure of the central value around which the\n","data tends to cluster. It is commonly used when the data is\n","approximately normally distributed or when the goal is to obtain a\n","representative value that takes all observations into account.\n","\n","**Median:** The median is the middle value in a dataset when it is\n","ordered in ascending or descending order. In other words, it divides the\n","dataset into two equal halves. If the dataset has an odd number of\n","observations, the median is the middle value. If the dataset has an even\n","number of observations, the median is the average of the two middle\n","values. The median is not influenced by extreme values or outliers and\n","provides a measure of the central value that is more robust to extreme\n","observations.\n","\n","To calculate the median, the dataset is first sorted, and then the\n","middle value or values are determined.\n","\n","**The choice between using the average or the median depends on the\n","nature of the data and the specific question being addressed:**\n","\n","-   The average is appropriate when the dataset is normally distributed\n","    or has no extreme outliers. It provides a representative value that\n","    takes into account the contribution of each observation.\n","\n","-   The median is useful when the dataset has extreme values or is\n","    skewed. It provides a measure that is less affected by outliers and\n","    is a better representation of the central tendency in such cases."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}