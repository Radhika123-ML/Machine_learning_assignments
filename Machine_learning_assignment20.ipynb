{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is the underlying concept of Support Vector Machines?**\n",
    "\n",
    "The underlying concept of Support Vector Machines (SVMs) is to find an\n",
    "optimal hyperplane that can separate different classes of data in a\n",
    "high-dimensional space. SVMs are a type of supervised machine learning\n",
    "algorithm used for classification and regression tasks.\n",
    "\n",
    "The key idea behind SVMs is to transform the input data into a\n",
    "higher-dimensional feature space using a mapping function. In this\n",
    "feature space, the algorithm aims to find a hyperplane that maximally\n",
    "separates the data points of different classes. The hyperplane is\n",
    "defined as the decision boundary that separates one class from another,\n",
    "and the optimal hyperplane is the one that maximizes the margin, which\n",
    "is the distance between the hyperplane and the nearest data points from\n",
    "each class.\n",
    "\n",
    "SVMs are effective in handling both linearly separable and non-linearly\n",
    "separable data. In cases where the data cannot be linearly separated,\n",
    "SVMs employ a technique called the kernel trick. The kernel trick allows\n",
    "SVMs to implicitly map the input data into a higher-dimensional space,\n",
    "where it becomes easier to find a hyperplane that separates the classes.\n",
    "\n",
    "The support vectors in SVMs refer to the data points that lie closest to\n",
    "the decision boundary, which are crucial for defining the hyperplane.\n",
    "These support vectors play a key role in determining the optimal\n",
    "solution and are used to make predictions for new, unseen data points.\n",
    "\n",
    "The main objective of SVMs is to maximize the margin and achieve a good\n",
    "generalization ability by minimizing the classification error. SVMs are\n",
    "known for their ability to handle complex data distributions and have\n",
    "been widely used in various domains, including text classification,\n",
    "image recognition, and bioinformatics.\n",
    "\n",
    "**Q2. What is the concept of a support vector?**\n",
    "\n",
    "In the context of Support Vector Machines (SVMs), a support vector\n",
    "refers to the data points that lie closest to the decision boundary or\n",
    "hyperplane. These support vectors are the critical elements in SVMs as\n",
    "they play a crucial role in defining the optimal hyperplane and making\n",
    "predictions.\n",
    "\n",
    "When training an SVM, the algorithm identifies the support vectors\n",
    "during the learning process. These support vectors have the property\n",
    "that they have a non-zero weight or influence on the position of the\n",
    "decision boundary. All other data points that are not support vectors\n",
    "have zero weights or do not affect the decision boundary.\n",
    "\n",
    "The selection of support vectors is determined by their proximity to the\n",
    "decision boundary. In a binary classification scenario, the support\n",
    "vectors are the points from both classes that are nearest to the\n",
    "hyperplane. These support vectors are typically the most informative\n",
    "data points as they lie at or near the margin, which is the region that\n",
    "separates the classes.\n",
    "\n",
    "Support vectors are essential because they define the decision boundary\n",
    "and play a significant role in the generalization ability of SVMs. Once\n",
    "the SVM is trained and the support vectors are identified, they are used\n",
    "to classify new, unseen data points. The distance of a new data point\n",
    "from the decision boundary can be calculated based on the distances from\n",
    "the support vectors. This distance can then be used to determine the\n",
    "predicted class label.\n",
    "\n",
    "It's worth noting that the number of support vectors is typically small\n",
    "compared to the total number of data points, especially when the data is\n",
    "well-separated or when using a high-quality kernel function. This\n",
    "property of SVMs makes them computationally efficient and\n",
    "memory-friendly, even with large datasets.\n",
    "\n",
    "**Q3. When using SVMs, why is it necessary to scale the inputs?**\n",
    "\n",
    "Scaling the inputs is necessary when using Support Vector Machines\n",
    "(SVMs) to ensure that all features contribute equally to the model's\n",
    "training process and avoid biased influence of certain features over\n",
    "others. **There are a few reasons why scaling is important in SVMs:**\n",
    "\n",
    "**1. Influence of feature scales:** SVMs aim to find an optimal\n",
    "hyperplane that maximally separates the classes. The decision boundary\n",
    "is sensitive to the scale of the features. Features with larger scales\n",
    "can dominate the optimization process and have a disproportionate\n",
    "influence on the placement of the hyperplane. By scaling the features,\n",
    "all of them are brought to a similar scale, preventing any single\n",
    "feature from overpowering the others.\n",
    "\n",
    "**2. Kernel function behavior:** SVMs often employ kernel functions,\n",
    "such as the radial basis function (RBF) kernel, to handle non-linearly\n",
    "separable data. These kernel functions compute the similarity or\n",
    "distance between data points. If the features have different scales, it\n",
    "can lead to incorrect or inconsistent similarity computations, affecting\n",
    "the accuracy of the SVM model. Scaling the inputs helps in maintaining\n",
    "the integrity of the kernel calculations.\n",
    "\n",
    "**3. Convergence and optimization:** SVMs optimize a cost function to\n",
    "find the best hyperplane. The optimization algorithms used in SVMs, such\n",
    "as gradient descent or sequential minimal optimization, converge faster\n",
    "and more reliably when the features are scaled. Scaling can help improve\n",
    "the convergence rate and avoid numerical instability during the\n",
    "optimization process.\n",
    "\n",
    "**4. Regularization parameter interpretation:** SVMs include a\n",
    "regularization parameter (C) that controls the trade-off between\n",
    "achieving a wider margin and allowing some misclassifications. The\n",
    "choice of C is influenced by the scale of the features. If the features\n",
    "are not scaled, it can lead to suboptimal C values and impact the\n",
    "model's performance.\n",
    "\n",
    "In summary, scaling the inputs is necessary in SVMs to ensure fair\n",
    "treatment of all features, maintain the integrity of kernel\n",
    "computations, improve convergence of optimization algorithms, and enable\n",
    "appropriate interpretation of regularization parameters. Common scaling\n",
    "techniques include standardization (mean centering and variance scaling)\n",
    "and normalization (scaling to a specific range, such as \\[0, 1\\]). The\n",
    "choice of scaling method depends on the specific characteristics of the\n",
    "data and the requirements of the SVM model.\n",
    "\n",
    "**Q4. When an SVM classifier classifies a case, can it output a\n",
    "confidence score? What about a percentage chance?**\n",
    "\n",
    "Yes, an SVM classifier can provide a confidence score or a measure of\n",
    "certainty for its predictions. However, unlike some other classifiers\n",
    "(such as logistic regression or decision trees), SVMs do not inherently\n",
    "provide probability estimates or direct percentage chances for\n",
    "classification.\n",
    "\n",
    "SVMs are primarily binary classifiers, meaning they are designed to\n",
    "classify data points into two classes. The decision boundary separates\n",
    "the two classes, and the SVM determines on which side of the boundary a\n",
    "given data point falls. The output of an SVM classifier is typically the\n",
    "predicted class label for a given input.\n",
    "\n",
    "However, you can estimate a confidence score or probability-like value\n",
    "from an SVM classifier using certain techniques. One common approach is\n",
    "to use the distance from the data point to the decision boundary as a\n",
    "confidence measure. The farther a point is from the decision boundary,\n",
    "the higher the confidence in its predicted class label. This distance\n",
    "can be calculated as the margin in the case of linear SVMs, or the\n",
    "output of the decision function for non-linear SVMs.\n",
    "\n",
    "It's important to note that these confidence scores from SVMs are not\n",
    "direct probabilities and do not necessarily represent percentage\n",
    "chances. They are relative measures that indicate the confidence or\n",
    "certainty of the classifier in its prediction. The interpretation and\n",
    "scaling of these confidence scores may vary depending on the specific\n",
    "implementation or post-processing techniques applied.\n",
    "\n",
    "If you require probability estimates or a percentage chance for\n",
    "classification, you can employ additional techniques such as Platt\n",
    "scaling or isotonic regression. These methods map the confidence scores\n",
    "of an SVM onto a probability scale using calibration techniques. By\n",
    "collecting a calibration dataset with true class labels, the confidence\n",
    "scores can be transformed into probability estimates.\n",
    "\n",
    "However, it's worth mentioning that if obtaining probability estimates\n",
    "is a crucial requirement, other classifiers such as logistic regression\n",
    "or ensemble methods like random forests may be more suitable as they\n",
    "inherently provide probability outputs.\n",
    "\n",
    "**Q5. Should you train a model on a training set with millions of\n",
    "instances and hundreds of features using the primal or dual form of the\n",
    "SVM problem?**\n",
    "\n",
    "When training a Support Vector Machine (SVM) model on a large dataset\n",
    "with millions of instances and hundreds of features, it is generally\n",
    "recommended to use the dual form of the SVM problem rather than the\n",
    "primal form.\n",
    "\n",
    "The dual form of the SVM problem is more suitable for large-scale\n",
    "datasets because it has better computational efficiency and memory\n",
    "requirements compared to the primal form. In the dual form, the\n",
    "optimization problem involves solving for a set of Lagrange multipliers\n",
    "associated with the training instances, rather than directly optimizing\n",
    "the weights and biases as in the primal form.\n",
    "\n",
    "**The advantages of using the dual form for large-scale datasets are:**\n",
    "\n",
    "**1. Computational efficiency:** The dual form involves solving a\n",
    "quadratic optimization problem that depends on the number of support\n",
    "vectors, which is typically much smaller than the total number of\n",
    "instances. This results in faster training times compared to the primal\n",
    "form, especially when dealing with millions of instances.\n",
    "\n",
    "**2. Memory requirements:** The dual form requires storing the kernel\n",
    "matrix, which is a matrix of size NxN (where N is the number of training\n",
    "instances). However, this matrix can be computed incrementally or\n",
    "approximated using techniques like the kernel trick, allowing for\n",
    "efficient memory usage compared to the primal form, which requires\n",
    "storing the feature vectors for each instance.\n",
    "\n",
    "**3. Flexibility in kernel functions:** The dual form naturally\n",
    "accommodates various kernel functions, including non-linear kernels such\n",
    "as the radial basis function (RBF). This flexibility is advantageous\n",
    "when dealing with high-dimensional data where a linear separation is not\n",
    "feasible.\n",
    "\n",
    "While the dual form is generally preferred for large-scale datasets, it\n",
    "is important to consider the specific characteristics of your dataset,\n",
    "such as the degree of separability and the computational resources\n",
    "available. For datasets with a small number of features or when the\n",
    "primal form offers computational advantages, it may still be a viable\n",
    "option. Ultimately, it is recommended to experiment and compare the\n",
    "performance and efficiency of both forms on your specific dataset to\n",
    "determine the most suitable approach.\n",
    "\n",
    "**Q6. Let's say you've used an RBF kernel to train an SVM classifier,\n",
    "but it appears to underfit the training collection. Is it better to\n",
    "raise or lower (gamma)? What about the letter C?**\n",
    "\n",
    "If an SVM classifier with an RBF kernel is underfitting the training\n",
    "data, there are adjustments that can be made to improve its performance.\n",
    "Specifically, the parameters gamma and C can be modified.\n",
    "\n",
    "**1. Gamma (γ):** The gamma parameter determines the influence of each\n",
    "training example in the computation of the decision boundary. A higher\n",
    "gamma value makes the decision boundary more focused on individual data\n",
    "points, potentially resulting in a more complex and flexible decision\n",
    "boundary. Conversely, a lower gamma value makes the decision boundary\n",
    "more spread out, considering a broader region around each data point.\n",
    "\n",
    "To address underfitting, you should consider increasing the gamma value.\n",
    "This makes the SVM classifier more sensitive to individual data points\n",
    "and can help capture intricate relationships in the training data.\n",
    "However, be cautious as increasing gamma too much can lead to\n",
    "overfitting, where the model becomes excessively sensitive to noise or\n",
    "specific instances in the training data.\n",
    "\n",
    "**2. C:** The C parameter controls the trade-off between the margin\n",
    "width and the number of training errors allowed. A smaller C value\n",
    "allows for a larger margin and allows more training errors\n",
    "(soft-margin), potentially resulting in a more generalized model. On the\n",
    "other hand, a larger C value makes the SVM classifier focus on\n",
    "minimizing training errors (hard-margin), leading to a narrower margin\n",
    "and potentially overfitting to the training data.\n",
    "\n",
    "If underfitting occurs, increasing the C value is typically recommended.\n",
    "This makes the SVM classifier pay more attention to minimizing training\n",
    "errors, potentially resulting in a more complex decision boundary that\n",
    "fits the training data better. However, similar to gamma, increasing C\n",
    "excessively can lead to overfitting.\n",
    "\n",
    "It's important to note that adjusting these parameters should be done\n",
    "carefully and in a controlled manner, ideally using cross-validation or\n",
    "a separate validation set to evaluate the model's performance. This\n",
    "allows you to find an optimal balance between model complexity and\n",
    "generalization. Additionally, other factors such as the dataset's\n",
    "characteristics and the number of training instances should also be\n",
    "considered when tuning gamma and C.\n",
    "\n",
    "Remember that finding the best parameter values might require\n",
    "experimentation and an iterative approach to fine-tuning the model.\n",
    "\n",
    "**Q7. To solve the soft margin linear SVM classifier problem with an\n",
    "off-the-shelf QP solver, how should the QP parameters (H, f, A, and b)\n",
    "be set?**\n",
    "\n",
    "To solve the soft margin linear SVM classifier problem using an\n",
    "off-the-shelf quadratic programming (QP) solver**, the QP parameters (H,\n",
    "f, A, and b) need to be set as follows:**\n",
    "\n",
    "**1. H (the Hessian matrix):**\n",
    "\n",
    "The Hessian matrix (H) is an NxN symmetric positive semi-definite\n",
    "matrix, where N is the number of training instances. For a linear SVM\n",
    "classifier, the Hessian matrix is defined as H = YY^T, where Y is an Nx1\n",
    "vector of training labels (with values +1 or -1). In other words, H\n",
    "represents the inner products of the training labels.\n",
    "\n",
    "**2. f (the linear coefficient vector):**\n",
    "\n",
    "The linear coefficient vector (f) is an N-dimensional vector that\n",
    "represents the linear term in the objective function. For a soft margin\n",
    "linear SVM classifier, f is set to be an N-dimensional vector of -1s, as\n",
    "the objective function aims to minimize the negative of the sum of the\n",
    "slack variables.\n",
    "\n",
    "**3. A (the matrix of linear equality constraints):**\n",
    "\n",
    "The matrix A defines the linear equality constraints. For a soft margin\n",
    "SVM classifier, A is an MxN matrix, where M is the number of\n",
    "constraints. In this case, M is equal to twice the number of training\n",
    "instances because there are two types of constraints: the upper bound\n",
    "and lower bound constraints on the slack variables. Each row of A\n",
    "represents a constraint and is constructed based on the training data.\n",
    "\n",
    "**4. b (the vector of linear equality constraints):**\n",
    "\n",
    "The vector b represents the right-hand side of the linear equality\n",
    "constraints. It is an M-dimensional vector, where M is the number of\n",
    "constraints. For a soft margin SVM classifier, b is an M-dimensional\n",
    "vector consisting of the upper bound and lower bound values for the\n",
    "slack variables.\n",
    "\n",
    "It's important to note that the exact formulation and organization of\n",
    "these parameters may vary depending on the specific QP solver being\n",
    "used. Some QP solvers might require the problem to be expressed in\n",
    "different forms or have specific requirements for the input format.\n",
    "Therefore, it is essential to consult the documentation of the specific\n",
    "QP solver you are using to ensure the parameters are correctly set\n",
    "according to its requirements.\n",
    "\n",
    "**Q8. On a linearly separable dataset, train a LinearSVC. Then, using\n",
    "the same dataset, train an SVC and an SGDClassifier. See if you can get\n",
    "them to make a model that is similar to yours.**\n",
    "\n",
    "Certainly! Let's proceed with training a LinearSVC, SVC, and\n",
    "SGDClassifier on a linearly separable dataset and evaluate if they\n",
    "produce similar models. **Here's an example implementation using\n",
    "scikit-learn in Python:**\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "**\\# Generate a linearly separable dataset**\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=2,\n",
    "n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "**\\# Split the dataset into train and test sets**\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42)\n",
    "\n",
    "**\\# Train LinearSVC**\n",
    "\n",
    "linear_svc = LinearSVC()\n",
    "\n",
    "linear_svc.fit(X_train, y_train)\n",
    "\n",
    "**\\# Train SVC**\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "**\\# Train SGDClassifier**\n",
    "\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2')\n",
    "\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "**\\# Make predictions**\n",
    "\n",
    "linear_svc_pred = linear_svc.predict(X_test)\n",
    "\n",
    "svc_pred = svc.predict(X_test)\n",
    "\n",
    "sgd_pred = sgd.predict(X_test)\n",
    "\n",
    "**\\# Evaluate accuracy**\n",
    "\n",
    "linear_svc_accuracy = accuracy_score(y_test, linear_svc_pred)\n",
    "\n",
    "svc_accuracy = accuracy_score(y_test, svc_pred)\n",
    "\n",
    "sgd_accuracy = accuracy_score(y_test, sgd_pred)\n",
    "\n",
    "**\\# Compare accuracies**\n",
    "\n",
    "print(\"LinearSVC Accuracy:\", linear_svc_accuracy)\n",
    "\n",
    "print(\"SVC Accuracy:\", svc_accuracy)\n",
    "\n",
    "print(\"SGDClassifier Accuracy:\", sgd_accuracy)\n",
    "\n",
    "\\`\\`\\`\n",
    "\n",
    "In this example, we first generate a linearly separable dataset using\n",
    "the \\`make_classification\\` function from scikit-learn. Then, we split\n",
    "the dataset into training and test sets. Next, we train a LinearSVC, SVC\n",
    "with a linear kernel, and an SGDClassifier with hinge loss (which\n",
    "approximates linear SVM). We make predictions on the test set and\n",
    "evaluate the accuracy of each model.\n",
    "\n",
    "By comparing the accuracies, we can assess if the LinearSVC, SVC, and\n",
    "SGDClassifier produce similar models for the given linearly separable\n",
    "dataset. However, please note that due to the stochastic nature of some\n",
    "algorithms (e.g., SGDClassifier), the exact results may vary across\n",
    "different runs.\n",
    "\n",
    "**Q9. On the MNIST dataset, train an SVM classifier. You'll need to use\n",
    "one-versus-the-rest to assign all 10 digits because SVM classifiers are\n",
    "binary classifiers. To accelerate up the process, you might want to tune\n",
    "the hyperparameters using small validation sets. What level of precision\n",
    "can you achieve?**\n",
    "\n",
    "Training an SVM classifier on the MNIST dataset, which consists of\n",
    "handwritten digit images, is a computationally intensive task,\n",
    "especially when using the one-versus-the-rest (OvR) approach to handle\n",
    "the multi-class classification problem. However, I can provide you with\n",
    "a high-level overview of the process and an estimation of the achievable\n",
    "precision.\n",
    "\n",
    "**Here's a general outline of the steps involved:**\n",
    "\n",
    "**1. Preprocess the data**: Preprocess the MNIST dataset by scaling the\n",
    "pixel values to a range suitable for SVMs, such as \\[0, 1\\]. You may\n",
    "also consider performing other preprocessing techniques, such as feature\n",
    "scaling or dimensionality reduction.\n",
    "\n",
    "**2. Split the data**: Divide the dataset into training, validation, and\n",
    "test sets. Since you mentioned using small validation sets to accelerate\n",
    "the hyperparameter tuning process, you can allocate a small portion of\n",
    "the data for validation.\n",
    "\n",
    "**3. Train an SVM classifier:** Utilize the OvR strategy by training ten\n",
    "separate binary SVM classifiers, where each classifier distinguishes one\n",
    "digit from the rest. You can use scikit-learn's \\`SVC\\` class and set\n",
    "the \\`decision_function_shape\\` parameter to \"ovr\" to enable OvR\n",
    "classification. Perform hyperparameter tuning using the validation set,\n",
    "optimizing parameters such as the regularization parameter C and the\n",
    "kernel function (e.g., linear, polynomial, or radial basis function).\n",
    "\n",
    "**4. Evaluate performance:** Evaluate the trained SVM classifier on the\n",
    "test set and compute the precision metric, which measures the proportion\n",
    "of correctly predicted positive instances (digits) out of the total\n",
    "predicted positive instances. Use scikit-learn's \\`precision_score\\`\n",
    "function to calculate precision.\n",
    "\n",
    "Now, regarding the achievable precision, SVM classifiers have shown\n",
    "excellent performance on the MNIST dataset, typically achieving\n",
    "precision values in the high 90s or even above 99%. However, the precise\n",
    "level of precision you can achieve depends on various factors, including\n",
    "the choice of hyperparameters, preprocessing techniques, feature\n",
    "engineering (if any), and the specific implementation details.\n",
    "\n",
    "To achieve the highest possible precision, it is crucial to perform\n",
    "comprehensive hyperparameter tuning, such as exploring different values\n",
    "of C and kernel parameters. Additionally, consider using advanced\n",
    "techniques like data augmentation, ensemble learning, or more\n",
    "sophisticated feature extraction methods to further enhance the model's\n",
    "performance.\n",
    "\n",
    "Remember that training an SVM classifier on the entire MNIST dataset\n",
    "with the OvR approach can be computationally demanding and\n",
    "time-consuming. Therefore, it might be beneficial to utilize\n",
    "computational resources, such as GPU acceleration or distributed\n",
    "computing, to expedite the training process.\n",
    "\n",
    "**Q10. On the California housing dataset, train an SVM regressor.**\n",
    "\n",
    "**Training an SVM regressor on the California housing dataset involves\n",
    "building a model that predicts the median house value based on various\n",
    "features. Here's an outline of the steps to train an SVM regressor on\n",
    "this dataset:**\n",
    "\n",
    "**1. Load and preprocess the data:** Load the California housing\n",
    "dataset, which typically includes features like average rooms,\n",
    "population, median income, etc., along with the corresponding target\n",
    "variable (median house value). Preprocess the data by performing\n",
    "necessary transformations, handling missing values, and scaling the\n",
    "features.\n",
    "\n",
    "**2. Split the data:** Split the dataset into training and test sets to\n",
    "evaluate the performance of the trained model.\n",
    "\n",
    "**3. Train the SVM regressor:** Use scikit-learn's \\`SVR\\` class to\n",
    "create an SVM regressor. Set the desired hyperparameters such as the\n",
    "kernel type (linear, polynomial, or radial basis function),\n",
    "regularization parameter C, and any other relevant parameters.\n",
    "\n",
    "**4. Fit the model:** Fit the SVM regressor to the training data using\n",
    "the \\`fit\\` method. This step involves finding the optimal decision\n",
    "function that minimizes the regression loss.\n",
    "\n",
    "**5. Evaluate the model:** Use the trained SVM regressor to make\n",
    "predictions on the test set. Evaluate the model's performance using\n",
    "appropriate regression evaluation metrics such as mean squared error\n",
    "(MSE), mean absolute error (MAE), or R-squared score. Calculate these\n",
    "metrics using scikit-learn's functions like \\`mean_squared_error\\`,\n",
    "\\`mean_absolute_error\\`, or \\`r2_score\\`.\n",
    "\n",
    "**Here's a code snippet that demonstrates the training and evaluation of\n",
    "an SVM regressor on the California housing dataset:**\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,\n",
    "r2_score\n",
    "\n",
    "**\\# Load the California housing dataset**\n",
    "\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "\n",
    "y = data.target\n",
    "\n",
    "**\\# Split the data into train and test sets**\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42)\n",
    "\n",
    "**\\# Create and train the SVM regressor**\n",
    "\n",
    "svm_regressor = SVR(kernel='linear')\n",
    "\n",
    "svm_regressor.fit(X_train, y_train)\n",
    "\n",
    "**\\# Make predictions on the test set**\n",
    "\n",
    "y_pred = svm_regressor.predict(X_test)\n",
    "\n",
    "**\\# Evaluate the model**\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "print(\"R-squared Score:\", r2)\n",
    "\n",
    "\\`\\`\\`\n",
    "\n",
    "Adjust the kernel type, hyperparameters, and evaluation metrics based on\n",
    "your specific requirements. Additionally, you can further enhance the\n",
    "model's performance through hyperparameter tuning, feature engineering,\n",
    "or using different kernel functions, depending on the characteristics of\n",
    "the California housing dataset."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
